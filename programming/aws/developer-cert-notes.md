AWS Certifcation Notes
==============

- [source](https://learning.oreilly.com/videos/aws-certified-developer)
- [certification documentation](https://aws.amazon.com/certification/certified-developer-associate/)  
- [certifcation study sources](https://aws.amazon.com/certification/certification-prep/)
- [practice test](https://www.aws.training/certification?src=exam-prep)

## Basics
- Cloud9 is a browser-based development environment for AWS
  - You have to provision an environment for it to run on remotely - usually just specifying a new EC2 instance
- Compute Fundamentals
  - EC2
    - Virtual Instance
    - Highly Configurable
      - OS
      - RAM
      - CPU
      - Many many options
  - Lightsail
    - Easier version of EC2
  - ECS
    - Container Orchestration
  - EKS
    - Container Service for Kubernetes
  - Lambda: Serverless
    - Pay only for execution time
  - Elastic Beanstalk
    - Rapid webservice / application development
- Storage Fundamentals
  - EBS: Elastic Block Store
    - Attach to EC2 instances
    - Easy to clone and snapshot 
  - S3
    - Object storage
  - EFS: Elastic Fileystem
    - Highly scalable
    - Mount on EC2
  - Glacier
    - Object storage like S3 but...
    - Long term storage 
- Physical Global Infrastructure
  - Availability Zones
    - Compute clusters which are connected with low latecny, high throughput, and redundant nw
    - You can deploy services into Multiple Availability Zones - to get HA / Fault Tolerance
  - Regions
    - Physically separated clusters of Availability Zones
  - Edge Locations
    - Servce requests for CloudFront (?)and Route 53 (?)
    - Sounds like a CDN
    - For cached content
- Shared Security Model (responsibilities)
  - "You": "In the cloud"
    - IAM
    - Security Groups
    - VPC (your network)
    - MFA
    - Key/Credential Rotation
    - OS patches
  - AWS: "Of the cloud" -> physical
    - Remember- except for managed services, they're still mostly IAAS
    - Physical services
    - DOOS mitigation
    - Personnel
- Resource Limits
  - Each account has limits such as the number of API keys or resources
  - You can request increases

### Lab Notes
- Creating a Cloud 9 instance to interact with your account
  - Services -> Cloud 9
  - It's important to remember which region you've selected for your Cloud9 instance
  - Notice the Cloud 9 URL: `https://us-east-1.console.aws.amazon.com/cloud9/ide/ce2cfd92a8904276803985fa507a2f77`
    - has region
    - has ID that is in the name of the associated instance
- Cloud9 Check Availability Zones
  - `aws ec2 describe-availability-zones`

## EC2
- Some basic EC2:
  - In the EC2 Dashboard, you can select 'Running Instances' and see a lot of interesting data on instances:
    - Instance State
    - Instance Type (EC2 type, e.g. micro, etc)
    - Elastic IP: public, static IP address which you can assign
    - Availability Zone
    - Security Groups: lets you view each FW group
    - AMI Id: Machine image ID used to provision instance
      - Note that AMI's are region-specific
    - Public DNS
    - Private DNS
    - VPC
    - Subnet
    - Block Device: you see the EBS device attached to the instance
  - within Images tab...
    - You can define your own
      - Elastic Block Store -> Snapshots -> Actions -> Create Image
      - You can also create snapshots of your volumes which is very useful
    - You can also view the available AMI's by clicking "Launch Instance"
  - Network & Security tab...
    - Elastic IP: You can create your own to assign to an EC2 instance
    - You only pay for the IP when you associate it to an EC2 instance
  - Instances have tags
    - You can associate any tags you want
  - VPC
    - It's like setting up a traditional network, but with the benefits of using the scalable infrastructure
  - Configuring an EC2 Instance
    - CloudWatch is a health monitoring service
    - Tenancy
    - Selecting Security Group
      - You select a connection type - e.g. SSH for linux, RDP for windows
      - You also select the source IP - i.e. where you're logging in from (so you can select your own IP address)
    - Finally, you create a keypair, and this only is downloadable once!
      - Once vm is provisioned, select it in dashboard and select 'connect'
      - Follow the instructions to ssh in
- Storage
  - Volume Types
    - EBS
      - General Purpose (gp2)
        - SSD
        - base performance of 3 IOPS per GB
        - burst to 3000 iops for extended periods
        - use cases: Boot volumes, small/med databases, dev/test environments
      - Provisioned IOPS SSD (io1)
        - You specify the IOPS you need, up to 32,000 IOPS
        - 500 MBs of throughput
      - Throughput Optimized HDD (st1)
        - Magnetic
        - Throughput: 500 Mb/sec
        - Low cost, sequential workloads
        - Use cases: EMR (map reduce), ETL, DW, log processing
      - Cold HDD (sc1)
        - Magnetic
        - Low cost, 250 MB/sec
        - Infrequent
    - EC2 Instance Store
      - Attached to the host computer
      - Emphemeral -lost when you stop the instance
      - buffers, temporary, etc.
    - EFS
      - Storage capacity is elastic
      - You can create this filesystem then the console will give you instructions how to mount the filesystem to your EC2 instance
        - be sure to match the security groups between EFS and EC2
      - Very good shareability across EC2 instances
- 'Bursting' vs 'Provisioned'
  - Bursting usually has a lower baseline with the ability to scale
  - Provisioned has a higher baseline but no additional headroom
- [Autoscaling and Load Balancing](https://aws.amazon.com/ec2/autoscaling/)  
  - **Summary of this section**:  LB -> TG -> ASG -> LC
    1. Create a 'Launch Configuration' which is just configuration which tells us how to spin up EC2 Instances
    2. Create an 'Auto Scaling Group' which defines the policies (we define) for scaling out (more instances) or scaling in (less instances), and associate it with our Launch Configuration
    3. Create a 'Target Group' for the Load Balancer to send traffic to
    4. Create a 'Load Balancer' which listens for requests (internal nw or public) and routes the traffic to a target group.
    5. Register targets (instances created by our Auto Scaling Group) with the target group  
  - Pretty much everything in here is provisioned with a wizard
  - In EC2 dashboard, go to "Auto Scaling"
    - `Launch Configuration`: Templates that an autoscaling group uses to launch EC2 instances
      - Create Launch Configuration -> Looks just like EC2 Creation Process except that you're not creating EC2 instances, but the configuration required to spin up new instances for the purpose of autoscaling
      - The next step is to assign a launch configuration to an  autoscaling group

    - `Auto Scaling Groups`: contains a collection of EC2 instances that share similar characteristics and are treated as a logical grouping for instance scaling and management 
      - You can associate an autoscaling group with a launch configuration (just mentioned) or a launch template (similar but adds versioning)
      - An autoscaling group includes:
        - Name
        - Instance Size
        - Network
        - Subnet
          - Note you can choose to scale into multiple subnets
        - Do load balancing
        - Other stuff too
      - Scaling Policies
        - 2 options:
          - Keep the group at it's initial size
          - Use scaling policies to define how to scale up  
        - You can set separate policies to scale out or in based on any number of server metrics
        - You can set alarms on scale actions by posting to (user-defined) topics in SNS (Amazon Notification Service - covered later)
  - `Target Groups`
    - Middle man between the Auto-scaling group and the Load Balancer
    - Each target group can only be associated with one Load Balancer
    - We also have to register targets with the target group
      - Go to the 'Targets' tab and select the instances associated with the autoscaling group
  - `Load Balancer`
    - Load Balancing -> Load Balancers
    - 3 Load Balancer Types
      1. Http(s)
        - Good for most application types: very flexible
        - Operate at request level
        - Good for microservices and containers too
      2. TCP
        - Ultrahigh performance
        - Applications require a static IP
        - Can handle millions of requests per second
      3. Classic Load Balancer: Old Version for backwards compatibility
    - Can be public or internal
    - Route Configuration step (in wizard)
      - Specify a target group, etc
    - When you've created a load balancer, you can view the details in the Listener tab
- EC2 Lab Objectives
  - Create an EC2 Instance of any type
  - Associate it with a new security group that only allows SSH traffic from your IP address
  - Create a snapshot of the EBS attached to the instance
    - Log in an touch a file
      - select instance
      - click `connect`
      - follow ssh instructions
      - touch a file in /home/ubuntu
    - 'Create Snapshot' in Elastic Block Store -> Snapshots -> Volume  (this takes a bit)
  - Create an AMI using the snapshot
    - From EBS Snapshot, choose 'Create Image'
    - Then go to Images -> AMI
  - Create a new instance using the AMI
  - Notes from the Walkthrough:
    - Image vs Instance
      - The image appears more oriented around the volume
      - You can still specify a variety of instance types for your image    
- Load Balancing Lab Objectives
  - Create a luanch configuration using a LAMP stack ami
  - Create an auoscaling group with the launch configuration
  - Create a load balancer and target group
  - Have the load balancer servce traffic to the instances in the autoscaling group
  - Verify you see a webapp when you navigate to the load balancers http endpoint

- Pricing models
  - On-Demand: You pay the minimal start up cost, but it gets more expensive as you have to increase capacity
  - Reserved Instances: You pay up front for a fixed capacity, cheaper if you know what you want
  - Dedicated Hosts: Most expensive up front

## S3
- Simple Storage Service
- Concepts
  - Buckets
    - Fundamental organization unit
    - Limitless size
    - Upload/download and permission
    - Every object is stored in a bucket
    - Provide namespacing
    - Identify account responsible / access control
    - Unit of aggregation for usage reports
  - Objects
    - Entities stored in buckets
    - 2 parts of data
      - Object data
      - metadata
        - name /value pairs
        - also includes:
          - date last modified
          - content type
          - custom pairs
    - identified by key and version id    
    - Can be up to 5TB  
  - Keys
    - Uniquely identify object in a bucket
    - Combo of bucket-key-version-id    
  - Regions
    - Choose the region the bucket lives in
    - Do this to:
      - Optimize latency
      - Conform to Regulatory requirements
      - Optimize costs
  - Data Consistency Model
    - Read-after-write
      - Inserting new objects (PUT)
    - Eventual
      - For Delete/Update
      - BC S3 data is replicated across multiple data centers
- Storage Classes
  - Frquently Accessed:
    - Standard
      - Default
    - Reduced_Redundancy (RRS)
      - Noncritical, reproducible data stored with less redundancy than standard storage class
      - Not generally recommended
  - Infrequently Accessed:
    - Standard_IA    
      - Stored across multiple availablility zones
    - OneZone_IA
      - Less expensive bc less redundant
    - **for both**: available quickly (ms) but there's an extra fee
    - good for older data that is accessed infrequently
  - Glacier
    - Archiving data
    - Not available for realtime
    - Needs to be restored before accessed
    - You transition normal S3 to Glacier using *Lifecycle Management* (covered later)
- Permissions Overview
  - All S3 resources are by default private 
    - Only resource owner can access initially
    - Resource owner can write an access policy to let others access
  - Access Policy Options
    - Resource Based: Policies you attach to your bucket, object, etc. (includes access control lists)
    - User Policies: Attaching policies to users in your account
    - *You typically use a combination* of each type
  - Access Control Lists (ACL's)
    - Specify r/w permissions to other **accounts**
    - Only accounts - not users in your account
    - No conditional permissions or deny-permissions
- Permissions
  - By default, all objects and buckets are private
    - only resource owner and the account that created it can access
    - but the resource owner can write an access policy
  - Access-Based Policy Types/Options
    - Resource-Based: any policies you attach your resource (object, bucket, etc.)
    - User-Based: policies attached to users in your account
  - ACL's (Access Control Lists)
    - Resource-based
    - Grant r/w permissions
    - Limits: 
      - permissions granted only to other AWS accounts, not users in your account
      - No conditional permissions
      - No explicit denials of permissions
    - Use cases:
      - Letting another account upload to a bucket
    - Bucket Permissions Section let you configure:
      - Your account
      - Other accounts
      - Public Access
      - S3 Log Delivery Group
      - Also have the ability to set a **Bucket Policy**
        - More detail later
        - JSON schema - Collection of "statements", each of which has
          - Resources: Buckets and Objects
            - use the `arn` (amazon resource name) format to identify the resource (e.g. bucket)
          - Actions
            - The permissions you allow or deny using action keywords
          - Effect
            - Either "Allow" or "Deny"
          - Principal
            - The account, user, service, etc that the policy applies to
- S3 Events
  - Triggers:
    - Newly created objects
    - Objects removed
    - Restore objects
    - Reduced Redundancy Class Object lost
    - Any replication events
  - Targets:
    - SNS
    - SQS
    - Lambda
- You can stream large S3 files
- Replication
  - Asynch copying of objects between buckets
  - Specify:
    - Whether Cross-Region-Replication (CRR) or Same-Region-Replication (SRR)
    - Destination Bucket
    - IAM role to assume during copying
    - Account to replicate to
  - Use Cases
    - Replication while maintaining metadata
    - Replicate into different storage classes
    - Replicate into different ownership
    - Replication within 15 minutes

## Cloud Formation
- Service that helps you set up your resources so you can spend less time on config, more on application
- Create a template with all the resources you want (e.g. EC2 instances, databases) and it helps you provision them, w appropriate config
  - Gives you a 'stack'
  - 'Infrastructure as Code'
- Manage a collection of resources as a single unit
- You can reuse the same template to provision into multiple regions
- You can track revisions to your template
- Templates
  - There are different formats
  - Resources appear as name-value pairs or objects

```(json)
"Resources": {
  "EC2Instance": {                //name of resource
    "Type": "AWS::EC2Instance",   //required field
    "Properties": {
      //...
    }, 
  },

  "InstanceSecurityGroup": {
    "Type": "AWS::EC2::SecurityGroup",
    "Properties": {
      //...
    }
  }
}

```

  - You can also use a YML syntax

- `Parameters` section let you define the input values (which will show up in the config UI)
  - You can associate descriptions and default values
- `Mappings` let you map from one exposed parameter value to a different value (analogous to name/value in an HTML `<select>`)
- Pseduo-Parameters are auto-resolved by AWS (basically keywords)
  - Ex: `AWS::Region` -> will resolve to whatever the region was active when the stack was created
- There are a variety of functions available in the template, including:
  - `Fn::GetAtt`  
  - `Fn::FindInMap`  (see bottom for more info)
- Outputs: ways to export different values when you instantiate using the template
  - use cases:
    - Good for organization
    - You can use the output from one to instantiate another stack
  - You can use `Fn:GetAtt` to pull out properties from particular resources
- Stacks and Stacksets
  - 2 ways to create a stack in the cloud formation services dashboard
    - Design Surface
    - Template (text sourced from...)
      - Local Computer
      - S3 location
      - Sample template
  - Dashboard Stack Creation
    - Name of stack
    - Parameters
      - Corresponds w parameters section of template
      - `AllowValues` -> dropdown selections
      - `KeyName` -> selects keypairs defined in that region
    - Add Tags
    - Create under a given IAM role
      - If you don't specify something, cloud formation will use the role defined in your specific account
    - Rollback Trigger
      - CF monitors the app while deploying and if it reaches a particular rollback threshold (e.g. monitoring time), it triggers a rollback
    - 'Termination Protection' is a configuration option which means you can't manually terminate the application from the dashboard
  - You can update a stack from the 'Update' menu
- Stacksets are a way for AWS to deploy a stack across multiple AWS accounts and Regions
  - You can identify account(s) (or upload a list of accounts with CSV's)
  - Alternately, you can deploy to Organizational Units (OU's) which are ways to organize multiple accounts within an AWS organization
    - Ex: you might have an OU for Production Application Accounts
  - 2 roles you need to specify:
    - IAM Admin Role: Role in the account you deploy FROM
      - has to have permissions to deploy into the child account
    - IAM Execution Role: Role in the account you deploy TO
      - has to have a role that allows external access from parent account
- AWS Systems Manager Parameter Store
  - basic idea
    - Secure, hierarchical storage for configuration and secrets management
    - good for passwords, secrets (tokens, keys, etc.), connection strings as parameter values
    - encoded as plain text or encrypted
    - you can refer to these parameters with the names assigned in the store
    - you can tag the parameters
    - you can also restrict access to these parameters
    - to consume a parameter for parameter store in your template, you define the type of the parameter to a reserved string (eg. AWS::SSM::Parameter::Name)
  - Navigate to 'Systems Manager' -> 'Parameter Store'
  - If you update a value in the Systems Manager Parameter Store, this will take effect when you click 'Update' on the consuming Cloud Formation Stack
- Wordpress Demo Notes (going through an existing template)
  - Refs can be pointing to Parameters or other resources
- Cloud Formation Lab Objectives
  - Deploy an EC2 Instance into a security group
  - The security group should allow SSH access from your machine
  - use Cloud 9
  - Find out where the schemas for various groups are
  - Parameters: keyname & ssh location
  - resources: EC2 and a new security group
  - Steps
    - Startup the Cloud9 EC2 Instance - not sure this is necessary
    - Log in to Cloud9 in that Environment
    - Select the sample LAMP stack template and download it, open in vscode
    - Create a second file, set language mode to json, type 'sta' and you'll get a cloud formation template skeleton
    - Go to the AWS Section on resource types to get the schema of the various types.
      - EC2
      - Security Group

## DynamoDB
- Features 
  - Seamless Scalability
  - Schemaless
  - No Admin Overhead (e.g. replication, clustering, hardware)
  - Encrypted
  - Easy monitoring and metrics dashboard
  - Easy/scheduled backups and point-in-time recovery (within last 35 days)
  - Built-in durability and high-availability
    - Automatically replicates data across 3 facilities in an AWS region
  - You can use global tables to keep regions in sync
- Basics
  - Collections are called 'Tables'
  - Records are called 'Items'
  - Fields are called 'Attributes'
    - nested attributes are ok up to 32 levels
- Keys
  - Each item has to be uniquely identified by a primary key
  - atrributes in primary key must exist in all items in a given table
  - 2 types of primary keys
    1. `Partition Key`
      - aka 'Hash Attribute'
      - Single attribute 
      - Plugs this value into a hash function which is used for partitioning/sharding
      - In a table with a Partition key, the values have to be unique

    2. `Composite Primary Key`
      - 2 attributes: Combination of a `Partition Key` and a `Sort Key`
      - Sort key is aka 'Range Attribute' 
      - Partition key still used for sharding, but it doesn't have to be unique
      - Values with same partition key are stored together but sorted by their sort key (which must be different)
  - Key-based queries, regardless of type, are supported
  - Each primary key attribute must be a scalar: string, number, or binary
  - Non-key attributes can be anything
- Secondary Index
  - You can create as many of these as you want
  - Lets you query against those additional attributes (named in the key)
  - 2 types of secondary indexes
    - Global: partition and sort key which can be different from the primary key
    - Local: same partition key as the primary key on the table but a different sort key
- Storage of Indexes
  - Each index gets stored (duplicated) in a separate table, but it is related to it's 'Base Table' (ie the original table)
  - Dynamo manages updating the index table as changes get made
  - When you create an index, you specify which other attributes should get projected from the base table
    - at a min, you get the table primary key attribute(s)
- Consistency Model
  - **Eventually Consistent Reads (default)**  (full consistency is usually within a second)
  - **Strongly Consistent Reads**
  - Transactions can be used when coordinating CRUD across multiple tables
- Read/Write throughput capacity
  - When you specify an index, you have to set throughput capacity which affects the cost
  - This is so AWS can reserve the appropriate resources to meet the capacity, of course
  - Read Capacity Unit (item <= 4KB size) represents either:
    - One strongly consistent read per second per Item
    - Two eventually consistent reads per second per Item
  - Write capacity unit represents:
    - One write per second for an item up to 1KB 
  - Size is cumulative: so 4 read capacity units lets you read strongly consistent item of 16KB/sec
  - If you exceed throughput, you'll be throttled by failing with a 400 code
  - Management Mechanisms
    - Auto Scaling
      - you define min/max and specify a target utilization rate
      - if you create through the dashboard, this is enabled by default
    - Provisioned Throughput
      - You just set the max throughput - no autoscaling
      - If you exceed, you get throttled
    - Reserved Capacity
      - you can purchase reserved capacity in advance and commit to a minimum
      - it's cheaper than autoscaling
  - Throughput calculations (for provisioned throughput) Examples:
    - Objective: strongly consistent read 80 items, each 3KB, per second from a table
      - Each read gets 1 read capacity unit bc 3KB/4KB rounds up to 1
      - 1 read capacity unit per item * 80 reads per second = 80 read capacity units
    - Objective: write 100 items (512 B) per second for a table
      - 512B / 1B rounds up to 1 capacity unit per item
      - 1 * 100 writes per second = 100 capacity units
  - Billing
    - You pre-defined (provision) your read/write throughput, after which you are billed by the hour when exceeding the free tier.
      - Although, you specify throughput on the table-level, you exceed the free tier when the cumulative amount goes over.
      - Min table throughput: 1 write CU and 1 read CU.
      - Total free tier threshold: 25 write CU, 25 read CU
    - So: it's a provisioned model.  You reserve in advance.  
    - But: you still have auto-scaling.  You can set a scaling policy such that a table scales up or down based on it's utilization.
    
- Point in Time Recovery
  - You can restore to any time in the last 35 days
  - Enable using the management console, dynamo API, or AWS API
  - Management console -> Table -> Backups -> Enable point in time recovery
  - If you enable it, that's when you start the clock for restore time
  - If you toggle it off/on, then you reset the clock
  - When you restore - it restores to a new table along with
    - Encryption settings
    - Any (global or local) associated secondary index tables
    - Read/write capacity settings
    - but:
      - you need to reset autoscaling
      - any other services, such as monitoring
- API's
  - [PutItem](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_PutItem.html)
  - [BatchWriteItem](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchWriteItem.html)
  - [GetItem](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_GetItem.html)
  - [BathGetItem](https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchGetItem.html)
    - You get up to 100 items from 1 (or more!) tables by specifying the corresponding ID's
  
## VPC
- You create a virtual network that resembles a physical network, but with the scalable infrastructure
- VPC is a Virtual Private Cloud
- It may share the same physical network as others but it is logically isolated
- You can
  - Specify an IP address range
  - Add subnets
  - Associate security groups
  - Configure route tables
- Subnets
  - A range of IP addresses inside your VPC
  - Use a **public subnet** for stuff which must be connected to the internet
  - Use a **private subnet** for stuff which shouldn't be publicly available
  - 2 methods for security
    - Security Groups
    - Network ACL's
- Every AWS account has a default VPC installed in each region
- Every VPC has a default subnet in each AZ in that region
- If you ignore the VPC's, then everything gets installed into your default VPC
  - However, you can create any number of non-default VPC's
- Non-default Subnets include:
  - additional subnets you create in your default VPC
  - any subnets you create in your non-default VPC
- You can control how resources in a VPC can access resources outside that VPC:
- Your default VPC contains an Internet Gateway
  - Internet Gateway lets instances communicate with internet through the Amazon EC2 Network Edge
- Each default subnet is a public subnet
  - Each instance you launch in in it:
    - has 2 IP addresses:
      - Private IPv4 address
      - Public IPv4 address
    - Can communicate with internet through the internet gateway
- Route table
  - Contains a set of rules used to determine where network is routed
  - Each subnet in your VPC has it's own route table which controls routing for the subnet
  - You can associate multiple subnets with 1 route table, but each subnet needs at least 1 route table
  - When you create a VPC, it automatically has a route table
    - In route tables page in AWS VPC dashboard, you can view the VPC-specific route table by looking for "Yes" in the main column
    - The main route table is the default route table for any subnets which don't have their own route table
  - [various use cases](https://docs.aws.amazon.com/vpc/latest/userguide/route-table-options.html)
- Internet Gateways
  - Make it possible for subnets and EC2 instances to be publicly routable
  - It's a horizontally scaled, redundant, HA VPC component that allows communication between EC2 instances and the internet
  - 2 purposes
    - To provide a target for your route table for internet traffic
    - To perform NAT for EC2 instances with a public IP address
  - So it's like a superset of the NAT Gateway (below)
  - They can support IPv4 and v6 traffic
- NAT Gateways
  - Let instances connect to the internet, but prevent the internet from initiating those connections
  - Instances in a private subnet can connect to the internet (e.g. for patches) but they need to access this through a NAT gateway 
    - That NAT Gateway needs to be hosted in a public subnet
  - A public subnet is just one which has a route table entry that goes directly to the internet gateway (which is the default)
  - A private subnet will only have a route to get to the public subnet, and specifically to the NAT Gateway
- Aside on the 0.0.0.0 address:
  - Called the 'wildcard address' or 'unspecified address'
  - If we're on a server, it means:
    - all IPv4 addresses on the given machine
  - If we're in a route table, it means:
    - The default route (i.e. it doesn't match one of the explicit entries)
- Security
  - Security Groups
    - Like a virtual fw to control inbound/outbound traffic for your instances
      - Each instance can have <= 5 SG's
    - Instance Level
      - If you don't specify an instance SG, it gets the VPC's default SG
      - They are associated with VPC's even though they're applied to the instance-level
    - "Allow" rules only    
  - Network ACL's
    - Optional
    - Subnet level
    - "Allow" and "Deny" rules
- Lab
  - Objectives
    - Create a new VPC in any region
    - Create 2 subnets
    - Using a NAT Gateway, an internet gateway, and route tables, make 1 subnet public and the other private
    - Create an EC2 instance using a LAMP AMI and deploy it into the new public subnet
  - Walkthrough notes

## IAM
- Identity and Access Management
- Web services that helps you control access to resources
- Root user
  - Has all permissions
  - You sign in with the account's email address
  - Don't use this for everyday tasks
  - AWS Guidance: Only use it to assign IAM users to do stuff
- Features
  - Shared Access: You can grant other people access to your resources
    - Granular Permissions
  - Secure Access to AWS resources: secure credentials
  - MFA
  - Identity Federation
  - Identity Infor for assurance
  - PCI DSS Compliance: credit card processing stuff
  - Eventually consistent because it's replicated for HA
  - Free
- Concepts
  - **Principle**: Entity that can make a request for an action/operation on an AWS resource
    - Ex: Root user, Users, roles, federated users, applications
  - **Request**: Combination of info including...
    - Actions or Operations:  CLI, API, or Console-based action
    - Resources: The thing that is being accessed
    - Principle: Including the policy
    - Environment Data: User agent, SSL, time of day, etc.
    - Resource Data: Could be a tag on the Resource, such as a Name metadata tag
  - **Authentication**
    - You need to be signed in to make a request (excluding some S3)
    - For the CLI, you need to specify your **Access Key** and your **Secret Key**
  - **Authorization**
    - Uses values from the request context to check for policies that apply to the request
    - Typically stored in Json docs
    - Several types of policies
      - Permissions Policies
        - Define permission for object to which they're attached
        - incld Identity Policies, Resource Policies, ACL's
      - Permissions Boundaries
        - Advanced feature
        - Limits the max permission a principle can have
  - **Actions or Operations**
  - **Resources**
- Flow:
  - Get Authenticated
  - Principal Makes Request
  - Authorization Phase
    - Evaluate Identity Based Policies and Resource Based Policies
  - Action Phase
    - Do the thing
- Users API Keys
  - Users are attached to permissions via Groups
  - Best Practice:  Use Groups to assign permissions to users
    - But you also have options to copy permissions from another user or assign directly
  - Managed Policies exist for standard permissions
    - Each of the standard permissions can be displayed with Json

    ```
    {
      "Version": "2010-10-17",
      "Statement": {
        "Effect": "allow",
        "Action": "*",
        "Resource": "*"
      }
    }
    ```

  - When you create a user, you select:
    - Programmatic Access
    - Console Access
  - New users also get their own sign-in link
  - You can create an IAM role with Admin permissions, but by default they can't access your billing dashboard
- Policies
  - There are 4 types
    - Identity-Based
      - Json docs which have permissions and you attach to a user (perhaps via a group)
      - It tells you who can do what on what resource
      - 2 types
        - AWS Managed (e.g. Administrator, Lambda Administrator)
        - User Managed (i.e. we create)
    - Resource-Based
      - These are attached to the resource instead of the user (example: S3)
      - No managed policies for these
      - **Important**
        - It's not enough to only assign permissions to a principal in a resource-based policy
        - You also have to use an identity-based policy to grant the principal access to the resource
    - Organization SCP
      - Applies to Organizational Units
    - Access Control Lists
      - Control what principals can access a resource
      - Doesn't use JSON
      - Used by S3, VPN, and some others
- Roles
  - Roles are sets of permissions
  - The permissions are attached to the role, not to the IAM user or group
  - Can be used by:
    - Iam user in the same (or different) account as the role
    - Web services like EC2
    - External user from a external identity provider
  - Types of Roles
    - Service Role
      - that's a role a service assumes in your account to perform actions on your behalf
      - most AWS services require you to assign a role which covers all the permissions it will need
      - cant be used to grant access to services in other accounts
      - you can create, modify, delete service roles from iam console
    - EC2 Service Role
      - For launching an EC2 instance that runs your application
      - Role is assigned to the EC2 instance when it's launched
      - AWS provides temporary security credentials for the role and the EC2 instance to run its applications
    - Service-Linked Role
      - Predefined by the service, along with with how you create, modify, and delete the role
      - Include all the permissions required to call other services on your behalf
      - Takeaway: these kinds of roles simplify setting up a service bc you don't have to manually add the necessary permissions
  - Go to AWS->IAM->Roles
    - You'll have pre-existing roles for things like services like autoscaling, cloud9, etc.
    - They each have "Trusted Entities": those are the associated service names
    - When you create a role for the EC2 trusted entity, it will automatically create an `instance profile`, which you can associate with the instance when you launch the instance
- Groups vs Roles
  - Groups: For Users
  - Roles: Attach to entities, like an EC2 instance
- Misc Notes
  - Actions are the atomic-level security information
    - They're available OOTB for the platform
  - A Permission connects an action to a resource
    - (...and other things, like conditions, effect (ie allow or deny))
  - A policy is a group of permissions
  - You can define your own policy or use pre-defined ones
  - The IAM Permissions Visual Editor is really useful (the alternative is using JSON editing directly)
  - Groups are collections of users
  - Groups are assigned policies (if you follow best practices)

## Lambda
- Serverless runtime
  - Different runtimes
  - scales automatically
  - no charge when not running
  - no provisioning
  - zero administration
  - you're only responsible for your code
- Lambda Function
  - Custom code and any libraries
- Event Source
  - AWS service like SNS that triggers your function
  - Could be API Gateway or things like DynamoDB even
- Downstream Resources 
  - Additional AWS services like S3 that your Lambda calls
- Log Stream
  - Lets you annotate your code to view logs that are generated
- Lambda Dashboard
  - Throttling
    - After a certain number of invocations, it'll be throttled (no other invocations will be allowed)
  - Available Actions
    - Publish a new version
    - Create an alias of the same lambda
  - Qualifiers
    - Let you refer to versions and aliases (see above)
    - The point is to have version control and test different versions of your lambda
  - Add Triggers
    - API Gateway
    - CloudFront
    - CloudWatch Events
    - CloudWatch Logs
- First time you try to test the Lambda, it asks you to set up Test Events
- Authoring methods
  - Adding code inline (web editor)
  - Uploading a zip file
  - Upload a file from S3
- 'Handler' is the lambda entry-point (which has to be registered/identified somehow)
- Environment Variables can be set/visible to your Lambda
  - Good for config settings
- Basic settings
  - you can specify the memory allocated to your lambda
  - timeouts setting
- You can specify which VPC/Subnet/SG your lambda belongs to
- Adding Packages / Libraries
  - Create a lambda package by uploading a zip or specifying an S3 target
  - Basically, you just zip up your entire project (including, for example, your expanded node_modules dir)
- Some SDK Notes
  - [boto docs](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) 
    - Boto is the Amazon Web Services (AWS) SDK for Python. It enables Python developers to create, configure, and manage AWS services, such as EC2 and S3. Boto provides an easy to use, object-oriented API, as well as low-level access to AWS services.
- [AWS SDK for Node](https://aws.amazon.com/sdk-for-node-js/)
  - New version is 3.x (currently developer preview) which includes typescript and has modules
  - [api documentation](https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/index.html)
  - usable for client apps as well as running on lambda

## Other Databses
- **RDS**: Relational Data Service
  - Set up, operate and scale a relational db in the cloud
  - Cost efficient, resizable capacity
  - Manages common administration tasks
  - You can independently scale these operations:
    - CPU
    - Memory
    - Storage
    - IOPS
  - Bc it's highly managed, you don't get any shell access to the service
  - Automated/manual backups/restore available
  - [Snapshotting feature](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CreateSnapshot.html)
    - You can take full or incremental snapshots
    - You can copy them (restore) into new or the same DBS
    - If the source DB is encrypted, so too is the snapshot
    - You can create copies of snapshots, and copy into other regions
    - You can have automated or manual snapshots
  - HA with failovers 
  - Use PostGres, MySql, Maria DB read-replicas
  - DB Instances
    - Can have multiple DB's in them
  - Create and modify a DB Instance
    - AWS Commandline
    - AWS RDS API
    - Management console
  - Engine Options
    - Amazon Aurora: PostgreSQL & MySQL compatible / 5x Throughput of MySql and 3x that of PostgreSQL
    - MySql
    - MariaDB
    - PostgreSQL
    - Oracle
    - Sql Server
  - Provisioning Options (Aurora)
    - Capacity
      - Provisioned: you manage the server instance sizes
      - Provisioned with Aurora Parallel Query Enabled: You provision but delegate analytic query optimizations to Aurora (ie managed)
      - Serverless: spec the max / min number of resources for a cluster
    - Cluster Name
    - Cluster Identifier
    - Master User Name
    - VPC
    - Subnet Group
    - Public accessibility
    - Availability Zone
    - VPC Security Group
    - Ports
    - Parameter Groups ( config you apply to multiple instances )
    - Encryption
    - Failover
    - Backup retention period
    - Types of Logs to deploy to CloudWatch
    - Maintenance windows
    - Upgrade policies
    - Deletion Protection
  - You can create any number of alarm types as well
- **RedShift**: Datawarehouse & Data Lake
  - Fully managed
  - Petabyte scale
  - Has Nodes in a cluster
  - Each cluster runs a RedShift engine and has one or more databases
  - **Value proposition**: 10x faster than other DB tech
  - Can cost 1/10 the cost of on-prem datawarehouses
- **Elasticache**
  - In-memory clone of memcached or redis
  - Fully-managed & scaleable


## KMS
- Key mgmnt store
- Note: Keys are region-specific
- Control who can access your master keys and thus gain access to your encrypted data
- Integrated with most other AWS services
  - Including CloudTrail to track who is using the various keys
- Actions include:     
  - Creating, Describing, Listing, Enabling, Disabling Master Keys
  - Creating, Viewing Grants and Access Control Policies for Master Keys
  - Encryption, Decryption Data
- Walkthrough: Create a KMS Key and using to encrypt a S3 Bucket
  - Goto IAM -> Encryption Keys -> Create Key
    - You can import a key or tell KMS to generate (preferred)
    - Choose Key Administrators (IAM users / roles who can administer the key through KMS)
    - CHoose Users/Roles that can use the key for encryption/decryption
  - Goto S3 -> Create Bucket 
    - Enable Default Encryption and choose the KMS key      

## SWF
- Simple Workflow Service
- Build applications that coordinate work across distributed components
- Task: Logical unit of work performed by a component of your application
- You specify workers
  - You implement them
  - Can run on EC2 instances or on-prem
- Tasks can be long running, or time-out, or require restarts, have a provisioned profile, etc.
- SWF service will allocate tasks to workers and tracks them, including their state
- To coordinate tasks, you write a program to get the latest state of each task from SWF, and use that to initiate new tasks
- Maintains an applications execution state so it's resilient to failures in individual components
- Use cases (some)
  - Media processing
  - Web application backends
  - Business Process Workflows
  - Analytics Pipelines
- Walkthrough steps
  1. Create a domain
    - Basically just a name for the WF
  2. Register workflow and activity types
    - Activities are individual components of a simple WF    
  3. Deploy
    - Specify keypairs and the EC2 instance type to use
  4. Run
  5. View in Console

## SQS
- [sqs faq](https://aws.amazon.com/sqs/faqs/)
- Simple Queue Service
- Secure, durable, HA, hosted queue for decoupling distributed component
- Security
  - You control who sends, receives message
  - Server-side encryption
- At-least-once and Only-Once (FIFO) modes available
- Scalable / transparent
- Customizable: each queue can be different (delays, use S3 or dynamo for larger items, split messages, etc.)
- 2 types: Trade off scalability for sequential consistency
  1. Standard
    - Unlimited Throughput
    - At least once delivery (ie the same message might get enqueued more than once)
    - Best effort ordering
    - Use case: when throughput is the most important
  2. FIFO
    - High throughput (3000 mesgs/sec)
    - Exactly once processing (ie no duplicate messages are enqueued ever)
    - First-in, First-out
    - Use case: order is important
- Walkthrough
  - When you name your queue, you should suffix the type of queue (e.g. fifo) to keep things clear
  - Configuration options (for a FIFO queue)
    - `Default Visibility Timeout`: the length of time that a message received from a queue will be invisible to other receiving components, after it is consumed once.
      - it's a mutex
      - Consumers have 2 operations: Read and Delete.  But the queue doesn't know if a consumer has successfull received or processed a message it has served on a read, so it's up to the consumer to delete it.  This timeout hides the message in the queue from other consumers while it waits for a Delete message from the given consumer.
      
      - Default value is 30sec
    - `Message retention period`: how long it holds a message before deleting it (1-14 days)
    - Maximum message size (256 KB is system max)
    - `Delivery Delay`: Amount of time to delay the first delivery of all messages added to the queue (freshness / cool-down period)
    - `Receive Message wait time`: max time that a long polling receive call will wait for a message to become available before returning an empty response
      - Long polling is a technique to reduce costs which avoids unnecessary calls to the q service (which usually result in empty response).  It only returns on timeout or when you have a message.
      - Short polling is the default
    - `Content-based de-duplication`: You set a de-dupe interval and any messages whose hashes (based on content) have already been seen, will not be delivered
    - `Dead-letter queues`: what to do with messages have been unsuccessfully processed
    - Server side encryption
      - whether you encrypt and how long you cache the KMS key
  - With queue actions in console, you can...
    - send a message
      - Message Group Id: Specifies that  message belongs in a specific message group.   
        - Messages in the same message group are always processed sequentially
          - It will only serve messages in the same message group to one consumer at a time
        - Messages in a different message group might be processed out of order
      - You can manually specify a deduplication id too (it doesn't have to be content-based)
    - view / delete a message
  - You can use the CLI / SDK to poll for messages too
    - Get the URL of the SQS
    - cli: `aws sqs receive-message --queue-url <url>`
- SQS has poison pill functionality - a standard way a of enqueuing a message which, when processed by the consumer will instruct the consumer to shut down/stop
- Vs SNS
  - SNS is a push to multiple subscribers
  - SQS is a polling model for decoupling send/receive components
  - But SQS can receive notifications from SNS
- Vs Kinesis Streams
  - Kinesis stream is real-time processing of big data
- Billing
  - per request plus data egress (if outside region)
  - Free Tier provides 1 million requests per month
- Enqueueing and Dequeueing
  - On enqueue: you'll get an ID which confirms it was delivered to the queue
  - On reading a message: you'll get a reciept handle that you have to provide when deleting the message
- Queue type (fifo vs standard) is ummutable
- Inflight messages: messages that a received by a consumer but not yet deleted.
  - There are system limits for this
- Messages can stay in the queue between 1-14 days (default is 4)
- Anonymous access is possible in a message queue
- [AWS MQ](https://aws.amazon.com/amazon-mq/) is to port your existing message queue to the cloud
  - Supports the industry-standard APIs and Protocols


## SNS
- Simple NOtification Service
- Coordinates and manages the delivery of messages to subscribing endpoints / clients
- 2 types of clients
  1. Publishers / Producers
  2. Subscribers / Consumers
    - E.G. Webservers, Email Addresses, SQS queues, Lambda functions
- Consumers use of a few transport protocol:
  - SQS
  - Http(s)
  - Email
  - SMS (messaging)
  - Lambda (when subscribed to topic)
- You deploy a particular SNS topic
  - Each topic has a particular URL endpoint for publishers AND subscribers
- Walkthrough
  - Publishing from the CLI (good for Cloud9): `aws sns publish --topic-arn <arn> --message file://message.txt`
- [tutorial notes](https://www.youtube.com/watch?v=PsJsP-7cydk)
  - **Publish**: A component `pushes` to an SNS topic
  - **Subscription**: registered consumer takes action
- Use case: "Fan Out Applications"
- You can have S3 Puts trigger an SNS topic
- DynamoDB table changes can trigger SNS topic
- Topic Configurations:
  - Who can publish or consume events
    - Topic owner
    - Everyone in the AWS Account
    - Specified AWS Accounts
    - Requesters with certain external endpoints
  - Retry Policy
    - If the execution of the consumer results in an exception, SNS considers this a fail and applies retry policy
    - Config: Max retries, delay timing, etc.
    - If you're invoking a Lambda, you can associate a DLQ
      - You set this up in the normal lambda configuration (like with memory)
      - Just specify the name/queue and it'll be connected

## API Gateway
- Create, publish, maintain, monitor, secure, and scale API's
- Bridges the external world with AWS resources through a restful api
- It doesn't handle the backends - only how you connect to them
- It covers 
  - authorization, access control
  - traffic mgmt
  - monitoring
  - version mgmt
  - sdk generation (<- Wow!)
- Key Components
  - API Gateway API: The end-points that connect to backing services
    - Collection can be deployed
    - Permissions to invoke: IAM roles or API Gateway Lambda Authorizers    
  - API Deployment: Snapshot of your API
  - API Endpoints: When you deploy your API Gateway to a specific region
  - Method Request:  Public interface of an API method (parameters & body) request
  - Method Response: Public interface that an API method returns

## Developer tools
- **CodePipeline**
  - CI/CD Tool
  - Starting with your source, you can build/test/release
  - You can have manual approvals gating actions if you want
  - You can integrate your favorite dev tools (source, dev, CI/CD) into Code Pipeline
  - Real-time dashboard, view build details, status, rerun
  - Each stage may end up using lots of different AWS resources (lambda, othrt stuff)
  - Stages use input and output artifacts that are stored in S3 buckets you choose when you create the pipeline
- **CodeCommit**
  - Private Git SCM in the cloud
  - Secure, highly scalable
  - Supports src code amd binaries
  - Pull Requests, reviews, notifications
  - Integrate well with Code Pipeline, IAM
  - Supports git commands and its own CLI
- **CodeDeploy**
  - Handles deployments to EC2 (incl. on-prem) and Lambda
  - Any types of app content supported:
    - Source code
    - Configuration files
    - Lambdas
    - Executables
    - Packages
    - Scripts
    - Multimedia
  - That content can be provided via
    - S3
    - Github repo
    - Butbucket repo
  - works on any dev/tst/prd environment
  - works on any scale of your infrastructure
  - maximizes availability in an in-place deployment
    - rolling update across ec2 instances
    - you can specify the number of instances to take offline for updates
  - same with a blue-green deployment
    - handles re-routing traffic, etc
  - rollbacks are supported
  - components
    - **deployment group**: deploys to a new group of resources described in this section
    - **deployments**: history of deployments, includes the application and an app-spec file
      - app-spec file describes how to deploy to the app instances in the deployment group
    - **deployment configurations**: specify deployment speed and the minimum number of instances that must be healthy at a given time
    - **application**: deployment groups and revisions
- **CodeStar**
  - AWS Project Template Service
  - Helps you set up a project with repos, build pipelines, infrastructure, etc.
- Blue/Green Deployments
  - Cutover traffic between 2 identical environments but with different versions
  - Cutover can be managed with
    - Route 53 DNS  
  - Advantages of Blue/Green Deployments
    - Easy rollback
    - Cloud provisioning of additional environment is inexpensive
    - Cloud provisioning of environments is easy with an immutable architecture approach
    - You use scaling groups to scale out and in (in either environment)
    - No downtime
    - Supports canary testing, where you shift over a portion of traffic to the new environment
    - Work well with CI/CD workflows
  - Services which Support Blue/Green
    - **Route 53**
      - DNS, HA, Global 
        - Routes based on geography, health checks, and latency
        - Let's you specify a shorter TTL, which allows quicker propagation of the new mapping
    - **Elastic Load Balancing**
    - **AutoScaling**
      - You can attach different launch configuration versions to enable auto-scaling
      - Also lets you put an instance in Standby mode, which is great in the event of a rollback
    - **Elastic Beanstalk**
      - OOB support of load balancing and autoscaling
    - **OpsWorks** 
      - (based on chef)
      - configuration management service
      - helps cloning entire environments for when you prepare blue/green environments
    - **Cloud Formation**
      - Infrastructure as code
      - Good automation for cutovers with route 53 or autoscaling
    - **CloudWatch**
      - Monitoring service
      - Collect metrics, generate alarms
  - Techniques
    - 1: Swapping the auto-scaling group behind an elastic load balancer
      - Initially, you have a blue auto-scaling group behing the load balancer
      - Then you have a green auto-scaling group which is staged with the new code
      - At deploy time, you attach the green asg to the load balancer
      - Bc the ELB uses a least-outstanding-requests algorithm, it will favor the new green environment initially
      - You can also control how quickly it cuts over by setting the initial size of the green ASG
      - Then you can remove the old EC2 instances, or better, put them in standby mode if you need a possible rollback
      - Other notes:
        - You set the maximum instance count in your ASG's
        - Any new instances are added to the ELB's pool, as long as they pass a health check
        - The health check can be a custom test or just a ping
        - When an instance fails a health check, it will be reported as 'out of service' and then replaced by the ASG, if configured as such
        - When you scale in a group, the load balancer will remove the instance from the pool and drain current connections before they terminate
    - 2: Updating ASG Launch configurations
      - ASG's have their own Launch configs (and only 1)
      - Launch configs include AMI, instance type, keypairs, security groups, block device mapping
      - You can replace the launch configuration associated with an ASG, such that it refers to new code
        - Old instances will not be removed but they won't be created either
        - When ASG replaces instances, it will remove those with the oldest launch configuration
      - You can scale the ASG to twice it's orig size once you've associated it with the new launch config
      - Then you shrink it back to the original size
    - 3: Swap the environment of an elastic beanstalk environment
      - Clone the existing environment and use Route 53 to point to a new ELB pointing to the new environment
    - 4. CLone a stack in OpsWorks and Update DNS
      - OpsWorks has the concept of stacks
        - 1 or more layers
        - Layer = set of 1 or more EC2 instances
  - Best Practices with Blue/Green
    - Exclude Schema changes from the application boundary
      - Schema changes should be backwards compatible (ie the old client code can use them)
  - When not to use B/G deployments
    - When the app components are in different zones
    - Schema changes which are coupled with code
    - When you have a 3rd party product which is designed with a different update process

## ECS 
- Elastic Container Service
- Helps you run, stop, and manage docker containers on a cluster 
  - With a LaunchType of Fargate, your clusters can be serverless
  - Otherwise, use a LaunchType of EC2, and you'll manage the cluster
- Based on resource needs, isolation requirements, etc. you can 'schedule' which tasks go on which EC2 instances
- Eliminates the need to operate your own cluster/configuration management or worry about scaling management infrastructure
- Regional service- you can use multiple AZ's in your cluster
- Clusters can be in a new or existings VPC's
- Task definitions and services define which containers to run
- Container images are stored in registries which may exist within (ECR) or outside (DockerHub) your AWS infrastructure
- Task Definition File
  - Json file
  - Describes up to 10 container images that form your application
  - Specify any number of parameters for your application (launch types to use, ports, data volumes, etc.)
  - The specific parameters you use depend on your launch type
- Task
  - The instantiation of a task definition in a cluster
  - You can specify the number of tasks that will run on your cluster (handled by the 'task scheduler')
- Cluser
  - Logical grouping of resources
- Container Agent
  - Runs on each EC2 instance in your cluster that reports on its' status/health/etc.
  - It also starts and stops tasks
- Microservices Notes
  - "Tasks" to ECS are "Pods" to Kubernetes
  - Schedulers maintain the service/task counts
  - Each microservice should have its own CI/CD pipeline
  - "Design for Failure"


## Elastic Beanstalk
- Quickly deploy and manage without worrying about infrastructure
- The system will handle scaling, load balancing, provisioning, health monitoring
- Support for all sorts of runtimes (go, java, node, python, .net, ruby, php)

## White Papers
- [Certified Developer Associate info](https://aws.amazon.com/certification/certified-developer-associate/)
- Read the whitepapers that are linked there, especially
  - Shared Security
  - IAM
  - Protecting data at rest
  - VPC infrastructure
  - Managing security monitoring
  - Pillars of the well architected framework
    - Operational Excellence
    - Security
    - Reliability
    - Performance Efficiency
    - Cost Optimization
  - Cloud Architecting Best Practices
  - CI/CD
  - Serverless
  - Blue/Green Deployment

## Well-Architected Framework
- [Corresponding White Paper](https://d1.awsstatic.com/whitepapers/architecture/AWS_Well-Architected_Framework.pdf)
- General Design Principles
  - Stop Guessing about capacity: just scale elastically
  - Test at Production Scale- spin up your production scale environment and then destroy it
  - Automate you infrastrcture changes, and log
  - Allow for Evolution of the system
  - Data-Driven Architecture: Collect data on your architecture choices and change
  - Improve through Game-Days: Simulate production events
- 5 Pillars of Well-Architected Framework
  - operational excellence
  - Security
  - Reliability
  - Performance Efficiency
  - Cost Optimization
- **Operational Excellence**  
  - Design Principles
    - Perform infrastructure operations as code
    - Annotated Documentation: should be created with each build
    - Make Frequent, small, reversible changes
    - Refine operations procedures regularly
    - Anticipate Failure
    - Learn from all Organization Failures
  - Best Practices (this is fluff)
    - Prepare
    - Operate
    - Evolve
  - Key Services  
    - CloudFormation
    - AWS Config
    - CloudWatch
    - Amazon ElasticSearch Service (Amazon ES): helps you analyze log data to gain actionable insights
- **Security**
  - Design Principles
    - Implement a strong identity foundation
      - Implement principle of least permission
      - Enforce a separation of duties
      - Eliminate long term credentials
    - Enable Traceability
      - Monitor, alert, and audit actions and changes to your environment in real-time
      - Integrate logs and metrics with systems to automatically respond
    - Apply Security at all levels
      - Not just the outer layer
    - Automate Security Best Practices
    - Protect data in transit and at rest
      - classify data into sensitivity levels and use mechanisms like encryption, tokenization, and access-control
    - Avoid manual handling of data in any way
    - Prepare for security Events
  - Best Practices
    - Identity and Access Management
      - Predefine Principals (e.g. users, groups, services, and roles) and assign them approprate policies
      - Programmatic access (e.g. API calls) should be performed with temporary and limited-priveledge credentials such as those issued by AWS Security Token Service
    - Detective Controls
      - processing logs, events, and monitoring that allows for auditing, automated analysis, and alarming      
    - Infrastructure Protection
    - Data Protection
    - Incident Response  
  - Key Services
    - AWS Cloud Trail and AWS Cloud Watch
      - CloudWatch is about activity of services and resources
      - CloudTrail is a log of ALL actions inside your cloud
    - AWS GuardDuty is a threat detection service that looks for unauthorized behaviors
    - S3 lets you log access requests
    - AWS Key Management Service (KMS) makes it easy to create/control encryption keys
- **Reliability**
  - Design Principles
    - Test Recovery Procedures
    - Automatically Recover from failures
    - Scale horizontally
    - Manage change in automation
    - Don't guess about capacity
  - Best Practices
    - Foundations
    - Change Management
    - Failure Management
  - Key Services
    - IAM
    - VPC
    - CloudTrail (records AWS API calls)
    - AWS Config
    - CloudFormat
    - Glacier
    - KMS
- **Performance Efficiency**
  - Design Principles
    - Use 3rd party services where there is lacking knowledge (e.g. NoSql, ML, media transcoding)
    - Deploy your solution in multiple regions
    - Use Serverless
    - Experiment often
    - Use the tech that best aligns with your use case
  - Best Practices
    - Selection
      - Multiple optimization approaches are often necessary to fit the system to the workload(s)
      - Compute
        - Instance vs Containers vs Functions
      - Storage Considerations
        - Access Methods:
          - block
          - file
          - object
        - Patterns of Access:
          - random
          - sequential
        - Throughput
        - Frequency of Access 
          - online
          - offline
          - archival
        - Frequency of Update
          - Write-Once Read-Many (WORM)
          - Dynamic
      - Database
        - Availability
        - Consistency
        - Partition Tolerance
        - Latency
        - Durability
        - Scalability
        - Query Capability
        - Other considerations
          - RDBMD vs NoSql
          - Search Engine (or data warehouse) instead of Database      
    - Review
    - Monitoring - use CloudWatch, and/or Kinesis/SQS/AWS Lambda
    - Tradeoff
      - Caching solutions
        - Elasticache
        - DynamoDB Accelerator (DAX)- distributed caching in front of Dynamo
      - CloudFront: CDN
  - Services
    - S3 lets you change from SSD to HDD
    - CloudWatch
    - RDS allows read-only replicas, Dynamo works at any scale
    - VPC Endpoints and AWS Direct Connect can reduce nw distance and jitter
    - AWS Snowball- Physical devices for transporting large datasets into the cloud
- **Cost Optimization**  (basically emphasizing using higher-level PAAS, FAAS, etc. over IAAS)
  - Design Principles
    - Adopt a consumption model
      - Rather than elaborate forecasting, just turn stuff off when you don't use it (like dev/test environments)
    - Measure Overall efficiency
    - Stop Spending mondy on data centers
    - Analyze and attribute expenditure
    - Use managed and application level services to reduce the cost of ownership
  - Best Practices
    - Expenditure Awareness
      - AWS Cost Explorer to track your spend 
      - AWS Budges lets you send notications if your usages  or costs are not inline with forecasts
      - Tag resources (esp S3 and EC2 instances) withthings like cost centers, workload names or owners   
    - Cost Effective resrouces
      - You might run a task on a smaller instance, which by virtue of taking longer, will incur higher cost than a larger instance
      - On-Demand Instances let you pay for compute by the hour
      - Reserved Intances let you resere capactity and off savings up to 75% of On-Demand
      - Spot instances lets you leverage unused EC2 capacity and offer savings up to 90% off On-Demand pricing
        - appropriate where system can tolerate a fleet of servers which individual servers can come and go dynamically, like stateless web servers, batch processing, or HPC and big data
    - Matching Supply and demand    
    - Optimizing over time
      - Be aware of new managed AWS services which provide additional savings
    - Key Services
      - AWS Cost Explorer
      - AWS Budgets
      - CloudWatch 
      - Trusted Advisor 
      - Aurora on RDS to remove licensing costs
      - Direct Connect + Cloud Front to optimize data transfer
      - AWS News Blog and What's New section on the website

## Useful Information
- **Secure Initial Account Setup**
  - Plan for Least Priviledges by attaching users to groups
    - Including at billing and account level
  - Monitor Users (see below for more info)
  - Use CloudFormation (or similar) to automatically standing up logging and monitoring features for new solutions
    - For consistency across multiple accounts too
  - IAM Best Practices
    - DOn't use the root account
    - Enable MFA for the root acount
    - Automate creation of IAM users/groups
    - Use AWS or User-managed policies to grant permissions
    - Use AWS Security Token Service if you have an existing identity federation provider to grant external identities secure access to AWS resources without creating IAM Users
  - Best Practices
    - Create a security email list
    - Create a SNS topic and subscribe the security email list to it
      - Follow this same procedure for billing too
    - Enable CloudTrail (in all region) which captures global (security) application events
      - Send logs to a S3 bucket that your security team owns
    - Create CloudWatch Alarms for security / nw related API activity
    - AWS Config: monitors history of resource fonciguration changes (for monitoring)
    - Use AWS Budgets
- **Proper IAM Setup**
  - These are techniques in addition to what was mentioned before
  - Define groups based on organizational role rather than technical commonality
  - Use MFA (something you have vs something you know) for admin or privileges accounts
  - Rotate credentials regularly
  - Use Managed Policies (presets) over group and user policies
  - Policies should be granular (each should have only a few permissions)
  - For EC2 credentials, use roles instead of access keys
    - IAM roles provide a temporary access/secret key to an instance, which are what you need to do manually anways
  - Use roles rather than user user credentials to grant cross-account access
    - You create a role and assign to another account
    - Then the account can delegate the role to any of its users
    - No need for keypair rotation, etc.
  - Use IAM conditions to make permissions more policies
    - Examples
      - Limiting developer IAM accounts to only work in a given subnet
      - Locking down admin accounts to only work from a specific IP range
      - Granting a permission for a specific time window
- **Security Logging Capabilities**
  - All AWS Services provides operational log files or metrics
  - Many AWS Services generate security log data
    - Audit logs for access
    - Configuration Changes
    - Build Events
  - Best Practices
    - Enable Audit logging wherever available
    - Use secure, durable storage for log files
    - Develop log lifecycle policies (storage, deletion, aggregation, etc.)
    - Analyze your logs 
  - CloudTrail
    - Provides a history of API calls for an account
    - Facilitates
      - Security Analysis
      - Resource Change Tracking
      - Compliance Auditing
    - Enable always!
    - Delivers log files to S3 every 5 min
    - Integrates with CloudWatch and Lambda
  - AWS Config (different from AWS Systems Manager Parameter Store)
    - Resource Inventory
    - Configuration Change Notification and History
    - Change snapshots are sent to S3 buckets
    - To enable in S3: goto Server Access Logging
  - Elastic Load Balancing, CloudFront, RDS, RedShift Logs
    - also can be turned on and drop data into S3 buckets
  - VPC Flow Logs
    - Captures IP traffic in and out of the VPC
    - Can be applied at the VPC, Subnet, or individual NIC-level
    - Stored with CloudWatch logs (not S3!)
    - Good for network debugging
  - Centrally Managed Log Data in AWS
    - Most of the services will log to S3 buckets
    - You choose the bucket and tell the service what prefix to use
    - CloudWatch provides an alternative central logging facility
      - has alerts
      - has built in analysis features and AWS partners have all sorts of additional options 
- **VPC Design Best Practices**
  - The variety of networking options in AWS can be challenging if you're not a nw pro
  - Universal NW Principles which apply...
    - Implement non-overlapping nw ranges for private (on-prem) nws to simplify ability to route bewteen remote nws
  - Ensure that VPC nw range doesn't overlap with your networks other private (on-prem) nw ranges
  - Don't allocate all your IP's at once - keep some capacity
  - Divide your VPC nw range evenly across all AZ's in a region
  - Create 1 subnet per AZ for each group of hosts that have unique routing requirements (e.g. public and private)
  - Size your VPC CIDR and subnets to support significant growth from expected workflow
  - Read the VPC docs for Amazon
- **Tagging Strategies** 
  - Implement automated tools to manage tags - see Resource Groups Tagging API
  - Better to have too many tags
  - Cost Explorer lets you break down costs by tag
  - Use a consistent naming / casing convention
  - IAM Conditions can include tags... ex...
    - Limit API calls to specific environments    
  - Some common examples of tags:
    - Technical Tags
      - Name
      - Application ID - identify disparate resources for an application
      - Application Role - function of a particular resrouce (e.g. web server, database, etc.)
      - Cluster - Resource farms that share a common configuration
      - Environment - dev/tst/prd
      - Version 
    - Tags for Automation
      - Date/Time - time a resource should be started, stopped, deleted, rotated, etc
      - Opt-in/Opt-out 
      - Security - whether to enable encryption, VPC Flow Logs, etc.
    - Business Tags
      - Owner
      - Cost Center/Business Unit
      - Customer
      - Project
    - Security Tags
      - Confidentiality
      - Compliance
- **CLI Tips**
  - [see ref](https://docs.aws.amazon.com/cli/latest/reference)
  - `aws [options] <command> <subcommand> [parameters]`
  - `[options]` will refer to your difference services (e.g. `ec2`)
  
## Practice Questions
- For Certified Developer Associate Exam
- AMI's are stored in a particular region
- IAM roles are available across all regions
- Cloud Formation templates can be launched in different regions, but if you're using a region-specific resource like AMI's it might require changes
- In SQS, after a maximum number of retries, you can send a message to a dead-letter queue
- S3 Overwrite PUT's and DELETE's: Eventual Consistency
- Concurrency model 
  - Optimistic Concurrency: checking a value upon save that it hasn't changed
  - Pessimistic Concurrency: locks the row or column
  - But DynamoDB doesn't support item locking
- SNS: Publish/Subscribe
- You can have an SQS subscribe to SNS events
- Use support center in AWS Console to increase limits
- Timeouts usually indicate a SG issue
- All subnets in a VPC have default routes to all other subnets in the VPC
- Todo Study
  - `Fn:FindInMap`
    - always looks in "mappings" section of template
    - is just an array of references: `{"Fn::FindInMap": [ "RegionMap", { "Ref": "AWS::Region }, "32"]}`
  - Conditional Writes in DynamoDB

## Blue / Green Deployment Solution
[whitepaper](https://d1.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf)
- One example:
  - Elastic Beanstalk solution which clones the current environment and dose an in-place installation on the old, then swaps again
  - AWS CodePipeline Pipeline is triggered when you deposit a new build on an S3 Bucket
  - Lambda functions handle cloning the EC2, to swap URL's, and terminating that cloned environment when complete
  - AWS CodeBuild projects can swap URL's also and run tests

## API Gateway vs Application Load Balancers
- API Gateway supports:
  - native IAM integration
  - Built-in Request validation, including qs parameters and headers
  - Web Sockets are supported
  - Configurable throttling limits
  - Lets you set a resource policy to allow IAM users in another account access to the API, based on Signature Vesion 4 protocols
  - Lets you deny traffic based on address IP or range
  - Allow private API traffic based on source VPC or VPC endpoint
  - See other notes...  
- Todo: more comparisons of ALB and API Gateway

## Amazon Athena
- Serverless querying of S3 buckets

## AWS Cognito
- https://aws.amazon.com/cognito/
- Provies sign-up, sign-in, and access control to web and mobile applications quickly and easily
- Also provides events, like post authentication triggers

## AWS Config vs CloudWatch vs CloudTrail
- [src1](https://www.gorillastack.com/news/cloudtrail-vs-cloudwatch/), [src2](https://www.gorillastack.com/news/aws-config-vs-cloudtrail/)
- Short version:
  - CloudWatch is health monitoring services (app/service oriented)
  - CloudTrail is a log of all API calls inside your environment 
  - AWS Config tells you the history of all your resource states
- CloudWatch Components
  - Metrics
  - Alarms
  - Logs
  - Events
- CloudTrail
  - Log of each APPI call in your Amazon environment
  - Events are written to S3 buckets
  - Data includes...
    - Request
    - Response
    - Identity of requestor
    - Source (AWs Console, CLI, AWS service, 3rd Party Application)
    - Types:
      - "Data Events": Operations within a resource
      - "Management Events": Configuration or security changes
  - Schema
    - `type`: what type of `userIdentity` triggered the event (e.g. IAM, Root, service, etc.)
    - `userIdentity` - ARN of the user or role
    - `eventSource` - service the request was made to
    - `eventTime`
    - `awsRegion`
    - `eventName`
    - `sourceIPAddress` - which IP the request was made from
    - `userAent` - how the call was made
    - `errorCode` - if any error occurred
    - `errorMessage` - more details on errors
    - `requestParameters` - any parameters from the request
    - `responseElements`
    - `requestId` - identifies the specific request, generated by the service itself
    - `eventId` - unique ID generated by CloudTrail
    - `apiVersion`
    - `managementEvent` - whether this was a management event
    - `readonly` - was the operation readonly?
    - `resources` - any resources connected to the event
    - `reciepientAccountId`
    - `serviceEventDetails`
    - `sharedEventId` 
    - `vpcEndpointId` - if requests were made from a VPC
  - Taking actions on CloudTrail Events
    - Install the CloudTrail Slack Bot to monitor
  - How to capture CloudTrail events?
    - Enable CloudTrail
    - Select an S3 bucket to store events
  - Common use case is to query your CloudTrail logs with Athena
  - Update frequency: 
    - Events collected in 15 minute windows
    - Events delivered to S3 in 5 minute windows
    - **however**- you can monitor Cloud**Trail** events on Cloud**Watch** Event Bus and use that to set alerts or trigger lambdas, etc
- CloudWatch
  - **CloudWatch Metrics**
    - Time series performance data about your AWS services and resources
    - Each event is time-stamped
    - [Metric filters](https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html)
  - **CloudWatch Alarms**
    - Based on a change in state or threshold of one or more CloudWatch Metrics
      - Each alarm can factor in up to 10 CloudWatch Metrics
    - Alarm Trigger Use-cases
      - Dashboard
      - EC2 Action
      - AutoScale
      - Send alarm data over an SNS topic, that have that SNS Topic triger a lambda
  - **CloudWatch Logs**
    - It's a way for to centralize, retain, access, and analyze logs
    - Integration examples:
      - **EC2**: use the logging agent to stream logs from the EC2 instances to CloudWatch
      - **Glue** (ETL Tool): Output of glue crawlers is written by default to CloudWatch
      - **Lambda**: all std out and error written to CloudWatch Log Streams
      - **Route 53**: All the requests can be logged in CloudWatch
    - **CloudWatch Events**
      - Events respond to particular resource actions and have a variety of targets (SNS, SQS, Lambda, etc.)
        - They also can follow a cron schedule
      - Different from Alarms bc
        - Alarms are only for sustained changes
        - Alarms are based on Metrics
      - Metrics are more application/system-leve and Events are more Resource-Level
- AWS Config
  - **Managed Config Rules**
    - Lets you specify custom rules to make sure your resources conform to (e.g. Iam policy, S3 bucket config, etc.)  
    - You can write your own and implement them in a lambda
    - Trigger on the change of status in your environment

## Parameter Store vs Secrets Manager vs KMS vs Vault (non AWS)
- [src](https://linuxacademy.com/blog/amazon-web-services-2/an-inside-look-at-aws-secrets-manager-vs-parameter-store/)

- Secrets Manager is a new service, which presently doesn't have a big advantage over parameter store (yet)
- KMS keeps the keys you use to encrypt values (secrets)
- Parameter store and Secrets Manager both intergrate with KMS and IAM to allow who can access a value and decrypt it (from KMS)
- Parameter stores lets you keep unencrypted or encrypted values wherase Secrets Manager only lets you store encrypted
- Parameter Store and Secrets Manager
  - Similarities:
    - Store values under a Name or Key (prefixable)
    - Values are up to 4096 char long
    - Values are referenceable in CloudFormation templates
  - Differences:
    - Parameter Store is free up to 10,000
    - SM integrates with RDS and lets you auto-rotate your keys
      - You can use lambda to rotate you keys for other (non-RDS) services
    - SM lets you share secrets across accounts
    - SM has an API for random generation of passwrods
- Vault
  - Lots of differences with Parameter Store and Secrets Manager
  - Not a managed service: you manage the application and it has tons of configurations
  - Integrates directly with TerraForm (though not as nicely as many would like)

## Step Functions
- Vs Series of Lambdas Connected by SQS
  - Step Functions gives you a defined backbone/orchestration to see the zoomed out picture
- Individual steps in a Step Function can be custom components (on prem or in cloud) or other AWS Services
  - You only need to communicate with custom components via HTTP 
- Vs Simple Workflow Service:
  - Use Step Functions (they're newer)
- Express workflows: high-event rates and short workflows
- Standard Workflows: long running auditable workflows
- Step functions can be triggered by API Gateway

## Serverless and VPC's
- There are a variety of performance issues connecting serverless components (DynamoDB, Lambda, etc.) and VPC's
- VPC's are infrastructure-level constructs which deal with performance, isolation, security concerns of primarily EC2 resources
- Serverless components, by default, at a completely different level of abstraction so they don't naturally fit into a VPC
  - Previous solutions involved for Dynamo using a NAT Gateway or creating a VPN
  - Lambda (see notes) creates an Elastic Network Interface, which has scaling/cold-start issues
- AWS has tools to connect serverless components into VPC's but it depends on the resource how effective they are
  - Still a lot of innovations going into this space

## CloudFormation Intrinsic Functions
- **Ref**
  - `{ "Ref" : "logicalName" }`
  - Returns the value of the specified parameter or resource (esp if a pseudo-parameter)

- **FindInMap**: this example will return `ami-0ff8a91507f77f867`
  - `{ "Fn::FindInMap" : [ "MapName", "TopLevelKey", "SecondLevelKey"] }`
    - MapName: RegionMap
    - TopLevelKey: AWS::Region PseudoParameter
    - SecondLevelKey: The main thing you're looking for
  - Ref
```(json)
{
  ...
  "Mappings" : {
    "RegionMap" : {
      "us-east-1" : { 
        "HVM64" : "ami-0ff8a91507f77f867", "HVMG2" : "ami-0a584ac55a7631c0c" 
      },
      "us-west-1" : { 
        "HVM64" : "ami-0bdb828fd58c52235", "HVMG2" : "ami-066ee5fd4a9ef77f1" 
      },
      "eu-west-1" : { 
        "HVM64" : "ami-047bb4163c506cd98", "HVMG2" : "ami-0a7c483d527806435" 
      },
      "ap-southeast-1" : { 
        "HVM64" : "ami-08569b978cc4dfa10", "HVMG2" : "ami-0be9df32ae9f92309" 
      },
      "ap-northeast-1" : { 
        "HVM64" : "ami-06cd52961ce9f0d85", "HVMG2" : "ami-053cdd503598e4a9d" 
      }
    }
  },

  "Resources" : {
    "myEC2Instance" : {
      "Type" : "AWS::EC2::Instance",
      "Properties" : {
        "ImageId" : { 
          "Fn::FindInMap" : [ 
            "RegionMap", 
            { 
              "Ref" : "AWS::Region" 
            }, 
            "HVM64"
          ]
        },
        "InstanceType" : "m1.small"
      }   
    }
  }
}
```

- **Fn::GetAtt**: gets an attribute from a resource
  - `{ "Fn::GetAtt" : [ "logicalNameOfResource", "attributeName" ] }`
  - example: eturns a string containing the DNS name of the load balancer with the logical name myELB.
    - `"Fn::GetAtt" : [ "myELB" , "DNSName" ]`
  
## Multi-Region Architectures
- (This is a big topic, and deserves more details, but at a superficially high level...)
- Why?
  - Reduced latency for global users
  - Local compliance on data storage
  - Regional Failover solutions
- 2 architectures
  - Completely Distinct, isolated instances in each region
  - Multi-tenancy: all users access same URL but system can redirect to closest region
    - Better option for accomplishing the 3 goals listed above
- Main challenge is dealing with data replication, bc services are stateless and be scaled out pretty easily.
  - First partition you data sets into those which can be region-specific and those which need replication across the entire architecture (master data)
  - For the ones which need replication, choose one master data region and set up read-only replicas for the other regions
  - Services can be instantiated in each region as appropriate
  - A service which writes master data and is deployed in the master data region can write directly to it.  Then this data will propagate to the other regions' replicas.
  - That same service in the non-master-data region would need to place a message on a queue in the master-data-region, which will be eventually processed and update the master data, and then this data will propagate back to the other regions.
- You can configure Route53 DNS to use Geoproximity routing to make sure users accessing the same URL will be routed to the appropriate region.

## CodeSuite Service Comparison
- [one blog](https://aws.amazon.com/blogs/devops/implementing-gitflow-using-aws-codepipeline-aws-codecommit-aws-codebuild-and-aws-codedeploy/)
- [some comparisons](https://blog.symphonia.io/posts/2019-01-21_continuous-integration-continuous-delivery-on-aws)
- Recall [Continuous Integration vs Continuous Delivery vs Continuous Deployment](https://www.atlassian.com/continuous-delivery/principles/continuous-integration-vs-delivery-vs-deployment)
  - CI: Automation of Merge/Build process.  Runs automated tests against the build.  Encourages early merging (trunk-level development), which prevents all sorts of issues.
  - Continuous Delivery: Automation of Deployment process.  Releases are batched.
  - Continuous Deployment: Each change (assuming it passes the required tests) is released.  Requires highest levels of testing infrastructure.
- CodeCommit: github-light
- CodeBuild vs CodePipeline
  - CodeBuild: 
    - CI Tool: Builds/Tests
    - You Specify:
      - where to get code
      - various scripts: build, test, package
    - It runs these stages in a container or EC2 AMI you specify
    - Pretty cheap and simple
    - Can be defined with CloudFormation
  - CodePipeline:
    - Full CD Pipeline / Orchestrator
    - Unlike CodeBuild, you don't give it scripts to run.  You give it a sequence of actions.
    - Example actions:
      - Source
        - EG integration with CodeCommit, GitHub
      - Build + Test
      - Deploy
        - EG CloudFormation
          - Might involve CodeDeploy under hood
      - Manual Actions
    - Can be defined with CloudFormation
    - CodePipeline can include CodeBuild, and CodeBuild can handle simple CI/CD.
      - Use CodePipeline when you want fancier stuff like parallel actions, retry, overlapping executions
    - Doesn't have branches
  - Both are not good with monorepos  
- CodeDeploy: deployment automation tool for EC2 centered
  - You can hook this into a larger CI/CD orchestrator like CodePipeline but CodePipeline can handle a much broader range of deployment targets (serverless, etc.) and actions
  - CodeDeploy also helps you track rolling updates and application health based on configurable rules
- CodeStar: wraps CodeCommit, CodeBuilt, CodeDeploy, and CodePipeline into a friendly-ish package
  - Integrates with JIRA
  - You only pay for the other services you tie into
  - Target EC2, Beanstalk, or Lambda deployments
  - Dashboard available

## Notes From Sample Questions
- Store database credentials in `AWS Secrets Manager`
- `AWS AppSync`
  - https://aws.amazon.com/appsync/
  - Build an API which combines data from multiple sources
  - Managed service
  - Uses GraphQL
  - Satisfies Real-Time Requirements
  - AppSync is different from API Gateway bc API Gateway only supports a standard REST API
- API Gateway
  - Users can create a WebSocket based API as a stateful frontend for an AWS service
  - Often compared to Application Load Balancers
  - You would NOT use graphql with API Gateway - only AppSync
- `CloudWatch Agent` can stream logs and metrics to CloudWatch
- S3 multipart upload API enables you to upload large objects in parts
- Elastic Beanstalk Configuration Changes
  - Managed with Environment Management Console
    - Compare to Create New Environment Page which 
  - Also supported by CloudFormation
  - Some config changes (e.g. Health Monitoring Checks) are propagated to running instances
  - Others (e.g. launch config, VPC settings) require terminating instances
    - **Rolling Updates**: to minimize downtime, AWS applies these patches to batches of instances
    - **Immutable Updates**: new instances are launched outside your ASG, and both new and old sets of instances serve traffic until the new set passes health checks, and is then moved into the original ASG, and the old instances are terminated
- Amazon DynamoDB Accelerator: in-memory cache to improve 10x performance improvement.  Good way to read-optimize your instance.
- Cloudwatch High Resolution Metrics are 1-second level granularity and alarms are 10-second
  - Standard Resolution alarms are 1-minute
- Don't use Recursion with Lambda
  - [other best practices](https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html)
- Elasticache
  - Supports Redis and Memcached
  - Typically placed between a backend server and a database
  - Caching Strategies
    - Lazy Loading:
      - Application requests data from cache and it is missing
      - Cache returns null to the application and then loads the corresponding data
      - Good bc it's easy to rebuild the cache and only caches stuff you use, saving memory
      - You could still have stale data if another process accesses the database 
        - You can address through TTL
    - Write-through
      - Any time a process needs to write data, it writes to the DB and then updates the cache with the corresponding data
      - So there's no stale data (compare to Lazy Loading)
      - Typically make the cache-call non-blocking      
  - Storing session state
    - Storing session data on the webserver means you need to have server affinity- which is hard as you scale out
    - Cookies are ok if you plan on not storing sensitive data.  Otherwise, this is a good use case for server-side info and elasticache.
- API Gateway Request Pipeline Stages:
  - Method Request
  - Integration Request
- CloudWatch
  - TODO: More tutorials here
    - metrics, concepts, alarms, application and OS logs
  - CloudWatch doesn't aggregate across regions
  - Cloud Watch Statistic Set is for a high volume metric where you only want to send some statistic (count, avg, etc.) to CW rather than individual data points (e.g. request time)
  - [Review this](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html)
  - [Container Insights - New integration options with ECS](https://aws.amazon.com/blogs/mt/introducing-container-insights-for-amazon-ecs/)
- CodeBuild concepts
  - TODO: Watch some tutorials
- If you want to assume an IAM Role (i.e. for service accounts), you'll want to use the STS (Secure Token Service) which will provide you with temporary credentials for your SDK calls


## Practice Test Notes
https://chercher.tech/aws-certification/aws-dva-c00-certified-developer-associate-practice-exam-set-2
https://chercher.tech/aws-certification/aws-dva-c00-certified-developer-associate-practice-exam-set-3
https://chercher.tech/aws-certification/aws-dva-c00-certified-developer-associate-practice-exam-set-4
https://aws.amazon.com/iam/faqs/

